# Trends in Natural Language Processing with Nasrin Mostafazadeh[Link](https://twimlai.com/twiml-talk-337-trends-in-natural-language-processing-with-nasrin-mostafazadeh/)## Key areas### Attention*  [Attention is not explanation](https://arxiv.org/pdf/1902.10186.pdf)### Attempts to reduce inherent bias in the data* [What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes](https://arxiv.org/pdf/1904.05233.pdf) 	* Names of the individuals already contain biases regarding gender & race. 	* Comes up with a new way to reduce the bias correlation.	* Name Embeddings that decorrelate the bias and name.### Role of large pretrained networks for NLP* [BERT from Transformer Models](https://arxiv.org/pdf/1810.04805.pdf): 	* Large pretrained language model.	* Great results for transfer learning. 	* Key paper of 2019. 	* Google Search itself incorporated BERT into their search.* [GPT-2](https://openai.com/blog/better-language-models/): Open AI  	* Trained on large corpus, able to generate really convincing paragraphs. 	* Zero shot generalization to other tasks.	* Too dangerous to release to public! Lots of controversy behind it.* Is generation of natural language correct path? How can we measure progress of these models?* [From ‘F’ to ‘A’ on the N.Y. Regents Science Exams: An Overview of the Aristo Project](https://arxiv.org/pdf/1909.01958.pdf): Can AI beat an eighth grader? 	* Through BERT and ROBERTA, the performance on this task went to 90% from 63%!### Others* [Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference](https://arxiv.org/pdf/1902.01007.pdf): 